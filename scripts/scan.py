import csv
import json
import re
import time
from datetime import datetime
from urllib.parse import urljoin, urlparse
import io

import requests
from bs4 import BeautifulSoup
import datetime as _dt

# =====================
#  ê¸°ë³¸ ì„¤ì •
# =====================

# ğŸ”— Google Sheets â†’ CSV ë§í¬ (ì§€ê¸ˆ ë„¤ ì‹œíŠ¸)
CSV_URL = "https://docs.google.com/spreadsheets/d/e/2PACX-1vRPol5yt4wsLuE8G-4lgzu1x2I9zo8dLRTHQQ3C7Pc5871wvpcQUHq6pLJS4FUcS05G86VLdKguSf9M/pub?gid=1024238622&single=true&output=csv"

# í˜ì´ì§€ ì „ì²´ì—ì„œ "ì„¸ì¼ ì¤‘ì¸ì§€" ê°ì§€í•˜ëŠ” í‚¤ì›Œë“œ
# ARCHIVE ëŠ” ê¸°ë³¸ ì„¸ì¼ í‚¤ì›Œë“œì—ì„œ ì œì™¸ (íŠ¸ë© ë°©ì§€)
GLOBAL_KEYWORDS = [
    "SALE", "SEASON OFF", "SEASONAL", "WINTER", "SUMMER", "SPRING", "FALL",
    "CLEARANCE", "FINAL", "LAST CHANCE", "OUTLET",
    "REFURB", "REFURBISHED", "B-GRADE", "SAMPLE",
    "UP TO", "%", "DEAL",
    "ì„¸ì¼", "í• ì¸", "ì‹œì¦Œì˜¤í”„", "í´ë¦¬ì–´ëŸ°ìŠ¤", "ì•„ìš¸ë ›", "íŠ¹ê°€", "ìµœëŒ€"
]

# ë§í¬ê°€ ì„¸ì¼ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±ì„ ë³´ëŠ” í‚¤ì›Œë“œ
LINK_SALE_KEYWORDS = [
    "SALE", "SEASON", "OFF", "CLEARANCE", "OUTLET",
    "REFURB", "DISCOUNT", "PROMOTION", "EVENT", "WINTER", "SUMMER",
]

# ì ˆëŒ€ ë“¤ì–´ê°€ë©´ ì•ˆ ë˜ëŠ” ë§í¬ (ë¡œê·¸ì¸, íšŒì›ê°€ì…, ì¹´íŠ¸ ë“±)
LINK_BLACKLIST = [
    "LOGIN", "LOG-IN", "SIGNIN", "SIGN-IN", "SIGNUP", "SIGN-UP", "REGISTER",
    "JOIN", "MEMBER", "MYSHOP", "MYPAGE", "MY PAGE",
    "CART", "BAG", "BASKET", "CHECKOUT", "ORDER",
    "ACCOUNT", "PROFILE",
    "PRESS", "STORY", "LOOKBOOK", "LOOK BOOK",
    "INSTAGRAM", "FACEBOOK", "YOUTUBE", "TWITTER",
    "KAKAO", "PF.KAKAO.COM"
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari"
}

# =====================
#  ë‚ ì§œ ê¸°ë°˜ ì„¸ì¼ ê¸°ê°„ íŒë‹¨
# =====================

# 1) "1.28 - 2.11" / "1.28~2.11"
# 2) "1ì›” 28ì¼ - 2ì›” 11ì¼" / "1ì›” 28ì¼~2ì›” 11ì¼"
DATE_RANGE_PATTERNS = [
    re.compile(r'(\d{1,2})[./]\s*(\d{1,2}).{0,40}?[-~â€“]\s*(\d{1,2})[./]\s*(\d{1,2})'),
    re.compile(r'(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼.{0,40}?[-~â€“]\s*(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼'),
]


def _extract_date_range_from_text(text: str):
    """
    í…ìŠ¤íŠ¸ì—ì„œ '1.28 - 2.11' / '1ì›” 28ì¼ - 2ì›” 11ì¼' ê°™ì€ íŒ¨í„´ì„ ì°¾ì•„
    (start_date, end_date)ë¥¼ date ê°ì²´ë¡œ ë¦¬í„´.
    ëª» ì°¾ê±°ë‚˜ ì´ìƒí•œ ë‚ ì§œë©´ None.
    """
    if not text:
        return None

    for pat in DATE_RANGE_PATTERNS:
        m = pat.search(text)
        if not m:
            continue

        sm, sd, em, ed = map(int, m.groups())
        today = _dt.date.today()
        year = today.year

        # ë§ì´ ì•ˆ ë˜ëŠ” month ê°’(13ì›”, 24ì›” ë“±)ì´ë©´ ê·¸ëƒ¥ ë²„ë¦°ë‹¤
        if not (1 <= sm <= 12 and 1 <= em <= 12):
            return None

        try:
            start = _dt.date(year, sm, sd)
            end = _dt.date(year, em, ed)
        except ValueError:
            # ì¼(day)ì´ 32ì¼ ì´ëŸ° ì‹ìœ¼ë¡œ ë§ì´ ì•ˆ ë˜ë©´ ì—­ì‹œ ë²„ë¦¼
            return None

        # ì—°ë§/ì—°ì´ˆ ê±¸ì³ ìˆëŠ” ê²½ìš° ëŒ€ëµ ì²˜ë¦¬ (ì˜ˆ: 12.20 - 1.10)
        if end < start:
            end = _dt.date(year + 1, em, ed)

        return start, end

    return None



def refine_status_with_dates(official_url: str, cur_status: str, timeout: int = 10) -> str:
    """
    í˜„ì¬ statusê°€ 'sale'ì¼ ë•Œë§Œ,
    ê³µí™ˆ HTMLì—ì„œ ë‚ ì§œ ë²”ìœ„ë¥¼ ì°¾ì•„ ì„¸ì¼ì´ 'upcoming' / 'sale' / 'nosale'ì¸ì§€ ë‹¤ì‹œ íŒë‹¨.
    ë‚ ì§œ ëª» ì°¾ìœ¼ë©´ ì›ë˜ status ìœ ì§€.
    """
    if cur_status != "sale":
        return cur_status

    try:
        resp = requests.get(official_url, headers=HEADERS, timeout=timeout)
        resp.raise_for_status()
        html = resp.text
    except Exception:
        # ê³µí™ˆì„ ëª» ë¶ˆëŸ¬ì˜¤ë©´ ê·¸ëƒ¥ ê¸°ì¡´ ìƒíƒœ ìœ ì§€
        return cur_status

    text = re.sub(r"\s+", " ", html)

    rng = _extract_date_range_from_text(text)
    if not rng:
        return cur_status

    start, end = rng
    today = _dt.date.today()

    if today < start:
        return "upcoming"
    if today > end:
        return "nosale"
    return "sale"


# =====================
#  Google Sheets â†’ í–‰ ì½ê¸°
# =====================

def fetch_rows():
    resp = requests.get(CSV_URL, timeout=20)
    resp.raise_for_status()

    rows = []
    f = io.StringIO(resp.text)
    reader = csv.DictReader(f)

    for row in reader:
        brand = (row.get("brand") or "").strip()
        url = (row.get("official_url") or "").strip()

        # brand / url ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if not brand or not url:
            continue

        enabled = (row.get("enabled") or "").strip().upper()
        if enabled and enabled != "TRUE":
            # enabled ì¹¸ì´ ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ë³¸ TRUE ì·¨ê¸‰
            continue

        rows.append({
            "brand": brand,
            "official_url": url,
            "logo_url": (row.get("logo_url") or "").strip(),
            "keywords_override": (row.get("keywords_override") or "").strip(),
            "sale_url_override": (row.get("sale_url_override") or "").strip(),
            "detector_group": (row.get("detector_group") or "A").strip().upper(),
            "manual_check": (row.get("manual_check") or "").strip().upper() == "TRUE",
            "notes": (row.get("notes") or "").strip(),
        })

    return rows


# =====================
#  ì„¸ì¼ ë§í¬ í›„ë³´ ì°¾ê¸°
# =====================

def find_sale_link(html: str, base_url: str, keywords):
    """
    í˜ì´ì§€ ì•ˆì˜ <a> íƒœê·¸ë“¤ ì¤‘ì—ì„œ
    'ì„¸ì¼ í˜ì´ì§€'ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë§í¬ë¥¼ ì ìˆ˜ ë§¤ê²¨ì„œ í•˜ë‚˜ ê³ ë¦„.
    """
    soup = BeautifulSoup(html, "html.parser")
    base_host = urlparse(base_url).netloc.split(":")[0]
    candidates = []

    for a in soup.find_all("a", href=True):
        href = (a.get("href") or "").strip()
        if not href:
            continue

        full_url = urljoin(base_url, href)
        parsed = urlparse(full_url)
        host = parsed.netloc.split(":")[0]

        # ì™¸ë¶€ ë„ë©”ì¸ ë§í¬ëŠ” ìŠ¤í‚µ
        if host and host != "" and host != base_host:
            continue

        low = full_url.lower()
        text = (a.get_text(" ", strip=True) or "").upper()
        target = full_url.upper()

        # ë¸”ë™ë¦¬ìŠ¤íŠ¸ë©´ ì œì™¸
        if any(b in text or b in target for b in LINK_BLACKLIST):
            continue

        score = 0

        # ì„¸ì¼ ê´€ë ¨ í‚¤ì›Œë“œ ë§ì„ìˆ˜ë¡ ì ìˆ˜ â†‘
        for kw in keywords:
            up = kw.upper()
            if up in text or up in target:
                score += 5

        # ì¹´í…Œê³ ë¦¬/ë¦¬ìŠ¤íŠ¸/ì»¬ë ‰ì…˜ í˜ì´ì§€ ì„ í˜¸
        if "cate_no=" in low or "category" in low or "collection" in low or "product/list" in low:
            score += 3

        # ë‹¨ì¼ ìƒí’ˆ í˜ì´ì§€ëŠ” ì‚´ì§ íŒ¨ë„í‹°
        if ("product/detail" in low or "product_no=" in low) and "list" not in low:
            score -= 2

        if score <= 0:
            continue

        candidates.append((score, len(text), full_url))

    if not candidates:
        return None

    candidates.sort(key=lambda x: (-x[0], x[1]))
    return candidates[0][2]


# =====================
#  ë¸Œëœë“œë³„ ì„¸ì¼ ê°ì§€
# =====================

def detect_sale_for_brand(row):
    brand = (row.get("brand") or "").strip()
    url = (row.get("official_url") or row.get("url") or "").strip()
    override = (row.get("keywords_override") or "").strip()
    sale_url_override = (row.get("sale_url_override") or "").strip()
    group = (row.get("detector_group") or "").strip().upper()
    manual_check = bool(row.get("manual_check"))

    # í‚¤ì›Œë“œ ì…‹ êµ¬ì„±
    keywords = GLOBAL_KEYWORDS[:]
    if override:
        # ì½¤ë§ˆ or | ë‘˜ ë‹¤ ì§€ì›
        tmp = override.replace(",", "|")
        for kw in tmp.split("|"):
            kw = kw.strip()
            if kw and kw.upper() not in [k.upper() for k in keywords]:
                keywords.append(kw)

    status = "error"
    matched_kw = None
    error_msg = None
    sale_url = None

    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        html = resp.text
        text_upper = re.sub(r"\s+", " ", html).upper()

        status = "nosale"
        for kw in keywords:
            if kw.upper() in text_upper:
                status = "sale"
                matched_kw = kw
                break

        # override ìˆìœ¼ë©´ ê·¸ ë§í¬ ìš°ì„ 
        if sale_url_override:
            sale_url = sale_url_override

        # override ì—†ê³ , ì„¸ì¼ë¡œ ê°ì§€ë˜ë©´ ì„¸ì¼ ë§í¬ í›„ë³´ íƒìƒ‰
        elif status == "sale":
            sale_url = find_sale_link(html, url, LINK_SALE_KEYWORDS)

        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ê³µí™ˆ
        if not sale_url:
            sale_url = url

    except Exception as e:
        error_msg = str(e)
        status = "error"
        sale_url = url  # ì—ëŸ¬ì—¬ë„ ê³µí™ˆì€ ìœ ì§€

    # ë‚ ì§œ ê¸°ë°˜ìœ¼ë¡œ 'upcoming' / 'nosale' ì—¬ë¶€ í•œ ë²ˆ ë” ì²´í¬
    status = refine_status_with_dates(url, status)

    return {
        "brand": brand,
        "official_url": url,
        "sale_url": sale_url,
        "status": status,
        "matched_keyword": matched_kw,
        "group": group or None,
        "manual_check": manual_check,
        "error": error_msg,
    }


# =====================
#  ë©”ì¸ ì‹¤í–‰ë¶€
# =====================

def main():
    rows = fetch_rows()
    now = datetime.utcnow().isoformat() + "Z"
    results = []

    for row in rows:
        res = detect_sale_for_brand(row)
        if res is None:
            continue
        res["checked_at"] = now
        results.append(res)
        time.sleep(1)  # ë„ˆë¬´ ë¹ ë¥´ê²Œ ë•Œë¦¬ì§€ ì•Šë„ë¡

    out = {
        "generated_at": now,
        "total_brands": len(rows),
        "brand_list": [r["brand"] for r in rows],
        "sales": results,
    }
    with open("docs/sales.json", "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    main()
