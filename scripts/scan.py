import csv, json, re, time
from datetime import datetime
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

# Google Sheets â†’ CSV ë§í¬
CSV_URL = "https://docs.google.com/spreadsheets/d/e/2PACX-1vRPol5yt4wsLuE8G-4lgzu1x2I9zo8dLRTHQQ3C7Pc5871wvpcQUHq6pLJS4FUcS05G86VLdKguSf9M/pub?gid=1024238622&single=true&output=csv"

# í˜ì´ì§€ ì „ì²´ì—ì„œ "ì„¸ì¼ ì¤‘ì¸ì§€" ê°ì§€í•˜ëŠ” í‚¤ì›Œë“œ
# ğŸ”¥ ARCHIVE ëºìŒ (ê¸°ë³¸ì ìœ¼ë¡œëŠ” ì„¸ì¼ë¡œ ì•ˆ ë³¸ë‹¤)
GLOBAL_KEYWORDS = [
    "SALE", "SEASON OFF", "SEASONAL", "WINTER", "SUMMER", "SPRING", "FALL",
    "CLEARANCE", "FINAL", "LAST CHANCE", "OUTLET",
    "REFURB", "REFURBISHED", "B-GRADE", "SAMPLE",
    "UP TO", "%", "DEAL",
    "ì„¸ì¼", "í• ì¸", "ì‹œì¦Œì˜¤í”„", "í´ë¦¬ì–´ëŸ°ìŠ¤", "ì•„ìš¸ë ›", "íŠ¹ê°€", "ìµœëŒ€"
]

# ë§í¬ê°€ ì„¸ì¼ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±ì„ ë³´ëŠ” í‚¤ì›Œë“œ (ì§€ê¸ˆì€ í¬ê²Œ ì•ˆ ì“°ì§€ë§Œ ìœ ì§€)
LINK_SALE_KEYWORDS = [
    "SALE", "SEASON", "OFF", "CLEARANCE", "OUTLET",
    "REFURB", "DISCOUNT", "PROMOTION", "EVENT", "WINTER", "SUMMER",
]

# ì ˆëŒ€ ë“¤ì–´ê°€ë©´ ì•ˆ ë˜ëŠ” ë§í¬ (ë¡œê·¸ì¸, íšŒì›ê°€ì…, ì¹´íŠ¸ ë“±)
LINK_BLACKLIST = [
    "LOGIN", "LOG-IN", "SIGNIN", "SIGN-IN", "SIGNUP", "SIGN-UP", "REGISTER",
    "JOIN", "MEMBER", "MYSHOP", "MYPAGE", "MY PAGE",
    "CART", "BAG", "BASKET", "CHECKOUT", "ORDER",
    "ACCOUNT", "PROFILE",
    "PRESS", "STORY", "LOOKBOOK", "LOOK BOOK",
    "INSTAGRAM", "FACEBOOK", "YOUTUBE", "TWITTER",
    "KAKAO", "PF.KAKAO.COM"
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari"
}

# === ë‚ ì§œ ê¸°ë°˜ ì„¸ì¼ ê¸°ê°„ íŒë‹¨ í—¬í¼ë“¤ ===
import datetime as _dt

# 1) "1.28 - 2.11" / "1.28~2.11" ê°™ì€ í˜•ì‹
DATE_RANGE_PATTERNS = [
    re.compile(r'(\d{1,2})[./]\s*(\d{1,2}).{0,40}?[-~â€“]\s*(\d{1,2})[./]\s*(\d{1,2})'),
    # 2) "1ì›” 28ì¼ - 2ì›” 11ì¼" / "1ì›” 28ì¼~2ì›” 11ì¼" ê°™ì€ í•œê¸€ í˜•ì‹
    re.compile(r'(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼.{0,40}?[-~â€“]\s*(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼'),
]

def _extract_date_range_from_text(text: str):
    """
    í…ìŠ¤íŠ¸ì—ì„œ '1.28 - 2.11' / '1ì›” 28ì¼ - 2ì›” 11ì¼' ê°™ì€ íŒ¨í„´ì„ ì°¾ì•„
    (start_date, end_date)ë¥¼ date ê°ì²´ë¡œ ë¦¬í„´.
    ëª» ì°¾ìœ¼ë©´ None.
    """
    if not text:
        return None

    for pat in DATE_RANGE_PATTERNS:
        m = pat.search(text)
        if m:
            sm, sd, em, ed = map(int, m.groups())
            today = _dt.date.today()
            year = today.year

            start = _dt.date(year, sm, sd)
            end = _dt.date(year, em, ed)

            # ì—°ë§/ì—°ì´ˆ ê±¸ì³ ìˆëŠ” ê²½ìš° ëŒ€ëµ ì²˜ë¦¬ (ì˜ˆ: 12.20 - 1.10)
            if end < start:
                # ì˜¤ëŠ˜ì´ ëë‚˜ëŠ” ë‹¬ë³´ë‹¤ ì•ì´ë©´, ì„¸ì¼ì´ ì´ì „ í•´ì—ì„œ ì´ì–´ì§„ ê±¸ë¡œ ê°€ì •
                end = _dt.date(year + 1, em, ed)

            return start, end

    return None

def refine_status_with_dates(official_url: str, cur_status: str, timeout: int = 10) -> str:
    """
    í˜„ì¬ statusê°€ 'sale'ì¼ ë•Œë§Œ,
    ê³µí™ˆ HTMLì—ì„œ ë‚ ì§œ ë²”ìœ„ë¥¼ ì°¾ì•„ ì„¸ì¼ì´ 'upcoming' / 'sale' / 'nosale'ì¸ì§€ ë‹¤ì‹œ íŒë‹¨í•œë‹¤.
    ë‚ ì§œë¥¼ ëª» ì°¾ìœ¼ë©´ ì›ë˜ statusë¥¼ ê·¸ëŒ€ë¡œ ëŒë ¤ì¤€ë‹¤.
    """
    if cur_status != "sale":
        return cur_status

    try:
        resp = requests.get(official_url, headers=HEADERS, timeout=timeout)
        resp.raise_for_status()
        html = resp.text
    except Exception:
        # ê³µí™ˆì„ ëª» ë¶ˆëŸ¬ì˜¤ë©´ ê·¸ëƒ¥ ê¸°ì¡´ ìƒíƒœ ìœ ì§€
        return cur_status

    # ê³µë°±ì„ ì •ë¦¬í•´ì„œ í•œ ì¤„ì§œë¦¬ í…ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê¸°
    text = re.sub(r"\s+", " ", html)

    rng = _extract_date_range_from_text(text)
    if not rng:
        return cur_status

    start, end = rng
    today = _dt.date.today()

    if today < start:
        return "upcoming"
    if today > end:
        return "nosale"
    return "sale"
# === ë‚ ì§œ í—¬í¼ ë ===


def fetch_rows():
    resp = requests.get(CSV_URL, timeout=20)
    resp.raise_for_status()
    lines = resp.text.splitlines()
    reader = csv.DictReader(lines)
    return list(reader)


def find_sale_link(html: str, base_url: str, keywords):
    """
    í˜ì´ì§€ ì•ˆì˜ <a> íƒœê·¸ë“¤ ì¤‘ì—ì„œ
    'ì„¸ì¼ í˜ì´ì§€'ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë§í¬ë¥¼ ì ìˆ˜ ë§¤ê²¨ì„œ í•˜ë‚˜ ê³ ë¦„.
    (ì§€ê¸ˆì€ UIì—ì„œ sale_urlì„ ì•ˆ ì“°ì§€ë§Œ, ë‚˜ì¤‘ì„ ìœ„í•´ ìœ ì§€)
    """
    soup = BeautifulSoup(html, "html.parser")
    base_host = urlparse(base_url).netloc.split(":")[0]
    candidates = []

    for a in soup.find_all("a", href=True):
        href = (a.get("href") or "").strip()
        if not href:
            continue

        full_url = urljoin(base_url, href)
        parsed = urlparse(full_url)
        host = parsed.netloc.split(":")[0]

        # ì™¸ë¶€ ë„ë©”ì¸ ë§í¬ëŠ” ìŠ¤í‚µ
        if host and host != "" and host != base_host:
            continue

        low = full_url.lower()
        text = (a.get_text(" ", strip=True) or "").upper()
        target = full_url.upper()

        # ë¸”ë™ë¦¬ìŠ¤íŠ¸ë©´ ì œì™¸
        if any(b in text or b in target for b in LINK_BLACKLIST):
            continue

        score = 0

        # ì„¸ì¼ ê´€ë ¨ í‚¤ì›Œë“œ ë§ì„ìˆ˜ë¡ ì ìˆ˜ â†‘
        for kw in keywords:
            up = kw.upper()
            if up in text or up in target:
                score += 5

        # ì¹´í…Œê³ ë¦¬/ë¦¬ìŠ¤íŠ¸/ì»¬ë ‰ì…˜ í˜ì´ì§€ ì„ í˜¸
        if "cate_no=" in low or "category" in low or "collection" in low or "product/list" in low:
            score += 3

        # ë‹¨ì¼ ìƒí’ˆ í˜ì´ì§€ëŠ” ì‚´ì§ íŒ¨ë„í‹°
        if ("product/detail" in low or "product_no=" in low) and "list" not in low:
            score -= 2

        if score <= 0:
            continue

        candidates.append((score, len(text), full_url))

    if not candidates:
        return None

    candidates.sort(key=lambda x: (-x[0], x[1]))
    return candidates[0][2]


def detect_sale_for_brand(row):
    brand = (row.get("brand") or "").strip()
    url = (row.get("official_url") or row.get("url") or "").strip()
    enabled = (row.get("enabled") or "TRUE").strip().lower()
    override = (row.get("keywords_override") or "").strip()
    sale_url_override = (row.get("sale_url_override") or "").strip()
    # group / detector_group ë‘˜ ë‹¤ ì§€ì›
    group = (row.get("group") or row.get("detector_group") or "").strip().upper()

    if enabled in ("false", "0", "no"):
        return None

    # í‚¤ì›Œë“œ ì…‹ êµ¬ì„±
    keywords = GLOBAL_KEYWORDS[:]
    if override:
        for kw in override.split("|"):
            kw = kw.strip()
            if kw and kw.upper() not in [k.upper() for k in keywords]:
                keywords.append(kw)

    status = "error"
    matched_kw = None
    error_msg = None
    sale_url = None

    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        html = resp.text
        text_upper = re.sub(r"\s+", " ", html).upper()

        status = "nosale"
        for kw in keywords:
            if kw.upper() in text_upper:
                status = "sale"
                matched_kw = kw
                break

        # 1) override ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ê·¸ ë§í¬ ìš°ì„ 
        if sale_url_override:
            sale_url = sale_url_override

        # 2) override ì—†ê³ , ì„¸ì¼ë¡œ ê°ì§€ë˜ë©´ ì„¸ì¼ ë§í¬ í›„ë³´ íƒìƒ‰
        elif status == "sale":
            sale_url = find_sale_link(html, url, LINK_SALE_KEYWORDS)

        # 3) ê·¸ë˜ë„ ì—†ìœ¼ë©´ ê³µí™ˆ
        if not sale_url:
            sale_url = url

    except Exception as e:
        error_msg = str(e)
        sale_url = url  # ì—ëŸ¬ì—¬ë„ ê³µí™ˆì€ ìœ ì§€

    # ë‚ ì§œ ê¸°ë°˜ìœ¼ë¡œ 'upcoming' / 'nosale' ì—¬ë¶€ í•œ ë²ˆ ë” ì²´í¬
    status = refine_status_with_dates(official_url, status)

    return {
        "brand": brand,
        "official_url": url,
        "sale_url": sale_url,
        "status": status,
        "matched_keyword": matched_kw,
        "group": group or None,
        "error": error_msg,
    }


def main():
    rows = fetch_rows()
    now = datetime.utcnow().isoformat() + "Z"
    results = []

    for row in rows:
        res = detect_sale_for_brand(row)
        if res is None:
            continue
        res["checked_at"] = now
        results.append(res)
        time.sleep(1)

    out = {"generated_at": now, "sales": results}
    with open("docs/sales.json", "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    main()
